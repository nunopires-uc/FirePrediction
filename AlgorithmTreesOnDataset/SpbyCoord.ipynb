{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "from geopandas.tools import sjoin\n",
    "import math\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import concurrent.futures\n",
    "\n",
    "dfTreesDRP = pd.read_csv('TreesPortugueseTerritoryDropped.csv')\n",
    "\n",
    "# Convert dfTreesDRP to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(dfTreesDRP, geometry=gpd.points_from_xy(dfTreesDRP.decimalLongitude, dfTreesDRP.decimalLatitude))\n",
    "\n",
    "# Create a spatial index\n",
    "sindex = gdf.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistanceTwoPoints(lat1, lon1, lat2, lon2):\n",
    "    R = 6371e3  # Radius of the Earth in meters\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(delta_phi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "\n",
    "def check_byCoord(row, precision=120):\n",
    "    lat1 = row['latitude']\n",
    "    lon1 = row['longitude']\n",
    "    \n",
    "    # Create a buffer around the point\n",
    "    point = Point(lon1, lat1)\n",
    "    buffer = point.buffer(precision)\n",
    "    \n",
    "    # Use the spatial index to find points within the buffer\n",
    "    possible_matches_index = list(sindex.intersection(buffer.bounds))\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    precise_matches = possible_matches[possible_matches.intersects(buffer)]\n",
    "    \n",
    "    # If there are no close points, return the existing scientificNames or an empty string\n",
    "    if precise_matches.empty:\n",
    "        return row['scientificNames'] if 'scientificNames' in row and not pd.isna(row['scientificNames']) else ''\n",
    "    \n",
    "    # Get the unique 'scien_yeartificName' values of the 5 closest points\n",
    "    precise_matches['distance'] = precise_matches.apply(lambda x: DistanceTwoPoints(lat1, lon1, x['decimalLatitude'], x['decimalLongitude']), axis=1)\n",
    "    close_points_df = precise_matches.sort_values('distance').iloc[:5]\n",
    "    unique_names = close_points_df['scientificName'].unique()\n",
    "    \n",
    "    # If 'scientificNames' exists and is not NaN or empty, append new unique names to it\n",
    "    if 'scientificNames' in row and not pd.isna(row['scientificNames']) and row['scientificNames'] != '':\n",
    "        existing_names = row['scientificNames'].split('; ')\n",
    "        new_names = [name for name in unique_names if name not in existing_names]\n",
    "        return row['scientificNames'] + '; ' + '; '.join(new_names)\n",
    "    else:\n",
    "        return '; '.join(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_year = 2023\n",
    "#dfFires = pd.read_csv(f\"Dataset/b{_year}.csv\")\n",
    "dfFires = pd.read_csv(f\"DatasetWTrees/PreviousVersions/{_year}_checklocality.csv\")\n",
    "dfTreesDRP = pd.read_csv('TreesPortugueseTerritoryDropped.csv')\n",
    "\n",
    "num_chunks = 20\n",
    "\n",
    "# Split the DataFrame into smaller chunks\n",
    "chunks = np.array_split(dfFires, num_chunks)\n",
    "\n",
    "# Apply the function to each chunk\n",
    "for i in range(num_chunks):\n",
    "    chunks[i]['scientificNames'] = chunks[i].apply(check_byCoord, axis=1)\n",
    "    empty_count = (chunks[i]['scientificNames'] == '').sum()\n",
    "    print(f'Empty count in chunk {i}: {empty_count}')\n",
    "    non_empty_count = (chunks[i]['scientificNames'] != '').sum()\n",
    "    print(f'Non empty count in chunk {i}: {non_empty_count}')\n",
    "\n",
    "    chunks[i].to_csv(f'DatasetWTrees/PreviousVersions/check_locality2/{_year}_chunk_{i}.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "dfFires = pd.concat(chunks)\n",
    "\n",
    "\n",
    "print(\"Locality\")\n",
    "empty_count = (dfFires['scientificNames'] == '').sum()\n",
    "print(empty_count)\n",
    "\n",
    "non_empty_count = (dfFires['scientificNames'] != '').sum()\n",
    "print(non_empty_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ori/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "_year = 2023\n",
    "#dfFires = pd.read_csv(f\"Dataset/b{_year}.csv\")\n",
    "dfFires = pd.read_csv(f\"DatasetWTrees/PreviousVersions/{_year}_checklocality.csv\")\n",
    "dfTreesDRP = pd.read_csv('TreesPortugueseTerritoryDropped.csv')\n",
    "\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    chunk['scientificNames'] = chunk.apply(check_byCoord, axis=1)\n",
    "    empty_count = (chunk['scientificNames'] == '').sum()\n",
    "    print(f'Empty count in chunk {i}: {empty_count}')\n",
    "    non_empty_count = (chunk['scientificNames'] != '').sum()\n",
    "    print(f'Non empty count in chunk {i}: {non_empty_count}')\n",
    "    return chunk\n",
    "\n",
    "num_chunks = 20\n",
    "\n",
    "# Split the DataFrame into smaller chunks\n",
    "chunks = np.array_split(dfFires, num_chunks)\n",
    "\n",
    "# Create a ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Use the executor to map the function to the chunks\n",
    "    chunks = list(executor.map(process_chunk, chunks))\n",
    "\n",
    "# Concatenate the chunks back into a single DataFrame\n",
    "dfFires = pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunk\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Split the DataFrame into smaller chunks\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray_split(dfFires, num_chunks)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Process each chunk sequentially\u001b[39;00m\n\u001b[1;32m     19\u001b[0m processed_chunks \u001b[38;5;241m=\u001b[39m [process_chunk(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def process_row(row):\n",
    "    return check_byCoord(row)\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        chunk['scientificNames'] = list(executor.map(process_row, chunk.to_dict('records')))\n",
    "    empty_count = (chunk['scientificNames'] == '').sum()\n",
    "    print(f'Empty count in chunk: {empty_count}')\n",
    "    non_empty_count = (chunk['scientificNames'] != '').sum()\n",
    "    print(f'Non empty count in chunk: {non_empty_count}')\n",
    "    return chunk\n",
    "\n",
    "# Split the DataFrame into smaller chunks\n",
    "chunks = np.array_split(dfFires, num_chunks)\n",
    "\n",
    "# Process each chunk sequentially\n",
    "processed_chunks = [process_chunk(chunk) for chunk in chunks]\n",
    "\n",
    "# Concatenate the processed chunks back into a single DataFrame\n",
    "dfFires = pd.concat(processed_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
