{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistanceTwoPoints(lat1, lon1, lat2, lon2):\n",
    "    R = 6371e3  # Radius of the Earth in meters\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(delta_phi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    return R * c  # Distance in meters\n",
    "\n",
    "\n",
    "def check_locality(row):\n",
    "    parish = str(row['parish']).lower()\n",
    "    district = str(row['district']).lower()\n",
    "    unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) & \n",
    "                              (dfTreesDRP['stateProvince'].str.lower() == district)]['scientificName'].unique()\n",
    "    return '; '.join(unique_names)\n",
    "\n",
    "\n",
    "def check_locality2(row):\n",
    "    concelho = str(row['municipality']).lower()\n",
    "    district = str(row['district']).lower()\n",
    "    unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(concelho, na=False)) & \n",
    "                              (dfTreesDRP['stateProvince'].str.lower().str.contains(district, na=False))]['scientificName'].unique()\n",
    "    \n",
    "    if 'scientificNames' in row and not pd.isna(row['scientificNames']) and row['scientificNames'] != '':\n",
    "        existing_names = row['scientificNames'].split('; ')\n",
    "        new_names = [name for name in unique_names if name not in existing_names]\n",
    "        return row['scientificNames'] + '; ' + '; '.join(new_names)\n",
    "    else:\n",
    "        return '; '.join(unique_names)\n",
    "\n",
    "def check_district(row, precision=120):\n",
    "    if 'scientificNames' in row and row['scientificNames'] != '':\n",
    "        return row['scientificNames']\n",
    "    \n",
    "    district = str(row['district']).lower()\n",
    "    lat1 = row['latitude']\n",
    "    lon1 = row['longitude']\n",
    "    \n",
    "    # Filter dfTreesDRP based on 'locality' and 'stateProvince'\n",
    "    filtered_df = dfTreesDRP[(dfTreesDRP['stateProvince'].str.lower() == district)]\n",
    "    \n",
    "    # Calculate the distance for each row in the filtered DataFrame\n",
    "    filtered_df['distance'] = filtered_df.apply(lambda x: DistanceTwoPoints(lat1, lon1, x['decimalLatitude'], x['decimalLongitude']), axis=1)\n",
    "    \n",
    "    # Filter the DataFrame based on the distance\n",
    "    close_points_df = filtered_df[filtered_df['distance'] < precision]\n",
    "    \n",
    "    # Get the unique 'scientificName' values\n",
    "    unique_names = close_points_df['scientificName'].unique()\n",
    "    \n",
    "    return '; '.join(unique_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_districtCoordSingular(row, precision=500):\n",
    "    if 'scientificNames' in row and row['scientificNames'] != '':\n",
    "        return row['scientificNames']\n",
    "    \n",
    "    district = str(row['district']).lower()\n",
    "    lat1 = row['latitude']\n",
    "    lon1 = row['longitude']\n",
    "    \n",
    "    # Filter dfTreesDRP based on 'locality' and 'stateProvince'\n",
    "    filtered_df = dfTreesDRP[(dfTreesDRP['stateProvince'].str.lower() == district)]\n",
    "    \n",
    "    # Calculate the distance for each row in the filtered DataFrame\n",
    "    filtered_df['distance'] = filtered_df.apply(lambda x: DistanceTwoPoints(lat1, lon1, x['decimalLatitude'], x['decimalLongitude']), axis=1)\n",
    "    \n",
    "    # Filter the DataFrame based on the distance\n",
    "    close_points_df = filtered_df[filtered_df['distance'] < precision]\n",
    "    \n",
    "    # If there are no close points, return an empty list\n",
    "    if close_points_df.empty:\n",
    "        return []\n",
    "    \n",
    "    # Sort the DataFrame by the 'distance' column\n",
    "    close_points_df = close_points_df.sort_values('distance')\n",
    "    \n",
    "    # Get the 'scientificName' and 'distance' of the 5 closest points\n",
    "    closest_points = close_points_df.iloc[:5][['scientificName', 'distance']].apply(tuple, axis=1).tolist()\n",
    "    \n",
    "    return closest_points\n",
    "\n",
    "\n",
    "def check_districtCoord(row, precision=500):\n",
    "    if 'scientificNames' in row and row['scientificNames'] != '':\n",
    "        return row['scientificNames']\n",
    "    \n",
    "    district = str(row['district']).lower()\n",
    "    lat1 = row['latitude']\n",
    "    lon1 = row['longitude']\n",
    "    \n",
    "    # Filter dfTreesDRP based on 'locality' and 'stateProvince'\n",
    "    filtered_df = dfTreesDRP[(dfTreesDRP['stateProvince'].str.lower().str == district)]\n",
    "    \n",
    "    # Calculate the distance for each row in the filtered DataFrame\n",
    "    filtered_df['distance'] = filtered_df.apply(lambda x: DistanceTwoPoints(lat1, lon1, x['decimalLatitude'], x['decimalLongitude']), axis=1)\n",
    "    \n",
    "    # Filter the DataFrame based on the distance\n",
    "    close_points_df = filtered_df[filtered_df['distance'] < precision]\n",
    "    \n",
    "    # If there are no close points, return an empty string\n",
    "    if close_points_df.empty:\n",
    "        return ''\n",
    "    \n",
    "    # Sort the DataFrame by the 'distance' column\n",
    "    close_points_df = close_points_df.sort_values('distance')\n",
    "    \n",
    "    # Get the unique 'scientificName' values of the 5 closest points\n",
    "    unique_names = close_points_df.iloc[:5]['scientificName'].unique()\n",
    "    \n",
    "    return '; '.join(unique_names)\n",
    "\n",
    "\n",
    "def check_byCoord(row, precision=120):\n",
    "    lat1 = row['latitude']\n",
    "    lon1 = row['longitude']\n",
    "     \n",
    "    # Calculate the distance for each row in the filtered DataFrame\n",
    "    filtered_df['distance'] = filtered_df.apply(lambda x: DistanceTwoPoints(lat1, lon1, x['decimalLatitude'], x['decimalLongitude']), axis=1)\n",
    "    \n",
    "    # Filter the DataFrame based on the distance\n",
    "    close_points_df = filtered_df[filtered_df['distance'] < precision]\n",
    "    \n",
    "    # If there are no close points, return the existing scientificNames or an empty string\n",
    "    if close_points_df.empty:\n",
    "        return row['scientificNames'] if 'scientificNames' in row and not pd.isna(row['scientificNames']) else ''\n",
    "    \n",
    "    # Sort the DataFrame by the 'distance' column\n",
    "    close_points_df = close_points_df.sort_values('distance')\n",
    "    \n",
    "    # Get the unique 'scientificName' values of the 5 closest points\n",
    "    unique_names = close_points_df.iloc[:5]['scientificName'].unique()\n",
    "    \n",
    "    # If 'scientificNames' exists and is not NaN or empty, append new unique names to it\n",
    "    if 'scientificNames' in row and not pd.isna(row['scientificNames']) and row['scientificNames'] != '':\n",
    "        existing_names = row['scientificNames'].split('; ')\n",
    "        new_names = [name for name in unique_names if name not in existing_names]\n",
    "        return row['scientificNames'] + '; ' + '; '.join(new_names)\n",
    "    else:\n",
    "        return '; '.join(unique_names)\n",
    "\n",
    "\n",
    "def checkNearestPoint(row):\n",
    "    if 'scientificNames' in row and row['scientificNames'] != '':\n",
    "        return row['scientificNames']\n",
    "    \n",
    "    district = str(row['district']).lower()\n",
    "    lat1 = row['latitude']\n",
    "    lon1 = row['longitude']\n",
    "\n",
    "    filtered_df = dfTreesDRP[(dfTreesDRP['stateProvince'].str.lower().str == district)]\n",
    "\n",
    "    filtered_df['distance'] = filtered_df.apply(lambda x: DistanceTwoPoints(lat1, lon1, x['decimalLatitude'], x['decimalLongitude']), axis=1)\n",
    "\n",
    "    filtered_df = filtered_df.sort_values('distance')\n",
    "    \n",
    "    # Get the 'scientificName' and 'distance' of the nearest point\n",
    "    nearest_name = filtered_df.iloc[0]['scientificName']\n",
    "    nearest_distance = filtered_df.iloc[0]['distance']\n",
    "    \n",
    "    return nearest_name, nearest_distance\n",
    "\n",
    "\n",
    "def checkNearestPointCoord(row, precision=120):\n",
    "    if 'scientificNames' in row and row['scientificNames'] != '':\n",
    "        return row['scientificNames']\n",
    "    \n",
    "    lat1 = row['latitude']\n",
    "    lon1 = row['longitude']\n",
    "    \n",
    "    \n",
    "    # Calculate the distance for each row in the filtered DataFrame\n",
    "    dfTreesDRP['distance'] = dfTreesDRP.apply(lambda x: DistanceTwoPoints(lat1, lon1, x['decimalLatitude'], x['decimalLongitude']), axis=1)\n",
    "    \n",
    "    # Filter the DataFrame based on the distance\n",
    "    close_points_df = dfTreesDRP[dfTreesDRP['distance'] < precision]\n",
    "    \n",
    "    # If there are no close points, return an empty string\n",
    "    if close_points_df.empty:\n",
    "        return ''\n",
    "    \n",
    "    # Sort the DataFrame by the 'distance' column\n",
    "    close_points_df = close_points_df.sort_values('distance')\n",
    "    \n",
    "    # Get the unique 'scientificName' values of the 5 closest points\n",
    "    unique_names = close_points_df.iloc[:5]['scientificName'].unique()\n",
    "    \n",
    "    return '; '.join(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ori/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 0: 100\n",
      "Non empty count in chunk 0: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 1: 94\n",
      "Non empty count in chunk 1: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 2: 100\n",
      "Non empty count in chunk 2: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 3: 100\n",
      "Non empty count in chunk 3: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 4: 94\n",
      "Non empty count in chunk 4: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 5: 111\n",
      "Non empty count in chunk 5: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 6: 104\n",
      "Non empty count in chunk 6: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 7: 103\n",
      "Non empty count in chunk 7: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 8: 98\n",
      "Non empty count in chunk 8: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 9: 90\n",
      "Non empty count in chunk 9: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 10: 93\n",
      "Non empty count in chunk 10: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 11: 93\n",
      "Non empty count in chunk 11: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 12: 97\n",
      "Non empty count in chunk 12: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 13: 90\n",
      "Non empty count in chunk 13: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 14: 98\n",
      "Non empty count in chunk 14: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 15: 92\n",
      "Non empty count in chunk 15: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 16: 94\n",
      "Non empty count in chunk 16: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 17: 88\n",
      "Non empty count in chunk 17: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 18: 95\n",
      "Non empty count in chunk 18: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/953739539.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  unique_names = dfTreesDRP[(dfTreesDRP['locality'].str.lower().str.contains(parish, na=False)) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 19: 82\n",
      "Non empty count in chunk 19: 42\n",
      "Locality\n",
      "1916\n",
      "583\n"
     ]
    }
   ],
   "source": [
    "_year = 2023\n",
    "dfFires = pd.read_csv(f\"Dataset/b{_year}.csv\")\n",
    "dfTreesDRP = pd.read_csv('TreesPortugueseTerritoryDropped.csv')\n",
    "print(len(dfFires))\n",
    "\n",
    "\n",
    "num_chunks = 20\n",
    "\n",
    "# Split the DataFrame into smaller chunks\n",
    "chunks = np.array_split(dfFires, num_chunks)\n",
    "\n",
    "# Apply the function to each chunk\n",
    "for i in range(num_chunks):\n",
    "    chunks[i]['scientificNames'] = chunks[i].apply(check_locality, axis=1)\n",
    "    empty_count = (chunks[i]['scientificNames'] == '').sum()\n",
    "    print(f'Empty count in chunk {i}: {empty_count}')\n",
    "    non_empty_count = (chunks[i]['scientificNames'] != '').sum()\n",
    "    print(f'Non empty count in chunk {i}: {non_empty_count}')\n",
    "\n",
    "    chunks[i].to_csv(f'/DatasetWTrees/PreviousVersions/check_locality/{_year}_chunk_{i}.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "dfFires = pd.concat(chunks)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Locality\")\n",
    "empty_count = (dfFires['scientificNames'] == '').sum()\n",
    "print(empty_count)\n",
    "\n",
    "non_empty_count = (dfFires['scientificNames'] != '').sum()\n",
    "print(non_empty_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFires.to_csv(f'DatasetWTrees/PreviousVersions/{_year}_checklocality.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ori/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty count in chunk 0: 90\n",
      "Non empty count in chunk 0: 35\n",
      "Empty count in chunk 1: 85\n",
      "Non empty count in chunk 1: 40\n",
      "Empty count in chunk 2: 91\n",
      "Non empty count in chunk 2: 34\n",
      "Empty count in chunk 3: 85\n",
      "Non empty count in chunk 3: 40\n",
      "Empty count in chunk 4: 85\n",
      "Non empty count in chunk 4: 40\n",
      "Empty count in chunk 5: 102\n",
      "Non empty count in chunk 5: 23\n",
      "Empty count in chunk 6: 98\n",
      "Non empty count in chunk 6: 27\n",
      "Empty count in chunk 7: 95\n",
      "Non empty count in chunk 7: 30\n",
      "Empty count in chunk 8: 92\n",
      "Non empty count in chunk 8: 33\n",
      "Empty count in chunk 9: 88\n",
      "Non empty count in chunk 9: 37\n",
      "Empty count in chunk 10: 76\n",
      "Non empty count in chunk 10: 49\n",
      "Empty count in chunk 11: 76\n",
      "Non empty count in chunk 11: 49\n",
      "Empty count in chunk 12: 82\n",
      "Non empty count in chunk 12: 43\n",
      "Empty count in chunk 13: 78\n",
      "Non empty count in chunk 13: 47\n",
      "Empty count in chunk 14: 88\n",
      "Non empty count in chunk 14: 37\n",
      "Empty count in chunk 15: 84\n",
      "Non empty count in chunk 15: 41\n",
      "Empty count in chunk 16: 86\n",
      "Non empty count in chunk 16: 39\n",
      "Empty count in chunk 17: 83\n",
      "Non empty count in chunk 17: 42\n",
      "Empty count in chunk 18: 91\n",
      "Non empty count in chunk 18: 34\n",
      "Empty count in chunk 19: 77\n",
      "Non empty count in chunk 19: 47\n",
      "Locality\n",
      "1732\n",
      "767\n"
     ]
    }
   ],
   "source": [
    "#1916\n",
    "#583\n",
    "\n",
    "_year = 2023\n",
    "#dfFires = pd.read_csv(f\"Dataset/b{_year}.csv\")\n",
    "dfFires = pd.read_csv(f\"DatasetWTrees/PreviousVersions/{_year}_checklocality.csv\")\n",
    "dfTreesDRP = pd.read_csv('TreesPortugueseTerritoryDropped.csv')\n",
    "\n",
    "num_chunks = 20\n",
    "\n",
    "# Split the DataFrame into smaller chunks\n",
    "chunks = np.array_split(dfFires, num_chunks)\n",
    "\n",
    "# Apply the function to each chunk\n",
    "for i in range(num_chunks):\n",
    "    chunks[i]['scientificNames'] = chunks[i].apply(check_locality2, axis=1)\n",
    "    empty_count = (chunks[i]['scientificNames'] == '').sum()\n",
    "    print(f'Empty count in chunk {i}: {empty_count}')\n",
    "    non_empty_count = (chunks[i]['scientificNames'] != '').sum()\n",
    "    print(f'Non empty count in chunk {i}: {non_empty_count}')\n",
    "\n",
    "    chunks[i].to_csv(f'DatasetWTrees/PreviousVersions/check_locality2/{_year}_chunk_{i}.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "dfFires = pd.concat(chunks)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Locality\")\n",
    "empty_count = (dfFires['scientificNames'] == '').sum()\n",
    "print(empty_count)\n",
    "\n",
    "non_empty_count = (dfFires['scientificNames'] != '').sum()\n",
    "print(non_empty_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFires.to_csv(f'DatasetWTrees/PreviousVersions/{_year}_checklocality2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1732\n",
    "#767\n",
    "\n",
    "\n",
    "_year = 2023\n",
    "#dfFires = pd.read_csv(f\"Dataset/b{_year}.csv\")\n",
    "dfFires = pd.read_csv(f\"DatasetWTrees/PreviousVersions/{_year}_checklocality2.csv\")\n",
    "dfTreesDRP = pd.read_csv('TreesPortugueseTerritoryDropped.csv')\n",
    "\n",
    "num_chunks = 20\n",
    "\n",
    "# Split the DataFrame into smaller chunks\n",
    "chunks = np.array_split(dfFires, num_chunks)\n",
    "\n",
    "# Apply the function to each chunk\n",
    "for i in range(num_chunks):\n",
    "    chunks[i]['scientificNames'] = chunks[i].apply(check_byCoord, axis=1)\n",
    "    empty_count = (chunks[i]['scientificNames'] == '').sum()\n",
    "    print(f'Empty count in chunk {i}: {empty_count}')\n",
    "    non_empty_count = (chunks[i]['scientificNames'] != '').sum()\n",
    "    print(f'Non empty count in chunk {i}: {non_empty_count}')\n",
    "\n",
    "    chunks[i].to_csv(f'DatasetWTrees/PreviousVersions/check_locality2/{_year}_chunk_{i}.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "dfFires = pd.concat(chunks)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Locality\")\n",
    "empty_count = (dfFires['scientificNames'] == '').sum()\n",
    "print(empty_count)\n",
    "\n",
    "non_empty_count = (dfFires['scientificNames'] != '').sum()\n",
    "print(non_empty_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(_year):\n",
    "    dfFires = pd.read_csv(f\"Dataset/b{_year}.csv\")\n",
    "    dfTreesDRP = pd.read_csv('TreesPortugueseTerritoryDropped.csv')\n",
    "    print(len(dfFires))\n",
    "\n",
    "    num_chunks = 20\n",
    "\n",
    "    # Split the DataFrame into smaller chunks\n",
    "    chunks = np.array_split(dfFires, num_chunks)\n",
    "\n",
    "    # Apply the function to each chunk\n",
    "    for i in range(num_chunks):\n",
    "        chunks[i]['scientificNames'] = chunks[i].apply(check_locality, axis=1)\n",
    "        empty_count = (chunks[i]['scientificNames'] == '').sum()\n",
    "        print(f'Empty count in chunk {i}: {empty_count}')\n",
    "        non_empty_count = (chunks[i]['scientificNames'] != '').sum()\n",
    "        print(f'Non empty count in chunk {i}: {non_empty_count}')\n",
    "\n",
    "    dfFires = pd.concat(chunks)\n",
    "\n",
    "    print(\"Locality\")\n",
    "    empty_count = (dfFires['scientificNames'] == '').sum()\n",
    "    print(empty_count)\n",
    "\n",
    "    non_empty_count = (dfFires['scientificNames'] != '').sum()\n",
    "    print(non_empty_count)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with Pool() as p:\n",
    "        p.map(process_year, [2023, 2024])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
